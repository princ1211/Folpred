{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkoN63M2DJLS",
        "outputId": "e555c15e-7b75-4b9b-ad27-e2caf08f017a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Abhishek\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tweepy\n",
        "import re\n",
        "import string\n",
        "from textblob import TextBlob\n",
        "import preprocessor as p\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz12fwJFDJLX"
      },
      "source": [
        "**AUTHENTICATE CREDENTIALS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ss2kMJnDJLY"
      },
      "source": [
        "We use Tweepy API to access Twitter and download tweets. Tweepy supports accessing Twitter via Basic Authentication and the newer method, OAuth. Twitter has stopped accepting Basic Authentication so OAuth is now the only way to use the Twitter API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6UwpcNYDJLZ"
      },
      "outputs": [],
      "source": [
        "consumer_key= '9oO3eQOBkuvCRPqMsFvnShRrq'\n",
        "consumer_secret= 'BMWGbdC05jDcsWU5oI7AouWvwWmi46b2bD8zlnWXaaRC7832ep'\n",
        "\n",
        "access_token='313324341-yQa0jL5IWmUKT15M6qM53uGeGW7FGcy1xAgx5Usy'\n",
        "access_token_secret='OyjmhcMCbxGqBQAWzq12S0zrGYUvjChsZKavMYmPCAlrE'\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "api = tweepy.API(auth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "724swKzADJLZ"
      },
      "outputs": [],
      "source": [
        "#file location changed to \"data/telemedicine_data_extraction/\" for clearer path\n",
        "congress_tweets = \"C:/Users/Abhishek/Election Twitter Sentiment analysis/congress_test.csv\"\n",
        "bjp_tweets = \"C:/Users/Abhishek/Election Twitter Sentiment analysis/bjp_test_new.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oorgr6zLDJLa"
      },
      "outputs": [],
      "source": [
        "#columns of the csv file\n",
        "COLS = ['id', 'created_at', 'source', 'original_text','clean_text', 'sentiment','polarity','subjectivity', 'lang',\n",
        "        'favorite_count', 'retweet_count', 'original_author', 'possibly_sensitive', 'hashtags',\n",
        "        'user_mentions', 'place', 'place_coord_boundaries']\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o90bWlODJLb"
      },
      "source": [
        "The data was gathered for the time between January, 2019 to April, 2019. We can obtain tweets for a particular time period that we want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE40a5qcDJLc"
      },
      "outputs": [],
      "source": [
        "#set two date variables for date range\n",
        "start_date = '2019-04-1'\n",
        "end_date = '2019-04-20'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1levpwjrDJLd"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbmRF6PGDJLd"
      },
      "source": [
        "It is necessary to clean the text data before we pass the data to our models. The below script will help us clean our tweets. We remove unnecessary elements like emoticons, stopwords, and special characters. Then we tokenize the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIonLn4RDJLe"
      },
      "outputs": [],
      "source": [
        "# Happy Emoticons\n",
        "emoticons_happy = set([\n",
        "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
        "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
        "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
        "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
        "    '<3'\n",
        "    ])\n",
        " \n",
        "# Sad Emoticons\n",
        "emoticons_sad = set([\n",
        "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
        "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
        "    ':c', ':{', '>:\\\\', ';('\n",
        "    ])\n",
        " \n",
        "#Emoji patterns\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        " \n",
        "#combine sad and happy emoticons\n",
        "emoticons = emoticons_happy.union(emoticons_sad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRyO8IdZDJLf"
      },
      "outputs": [],
      "source": [
        "#mrhod clean_tweets()\n",
        "def clean_tweets(tweet):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(tweet)\n",
        " \n",
        "    #after tweepy preprocessing the colon left remain after removing mentions\n",
        "    #or RT sign in the beginning of the tweet\n",
        "    tweet = re.sub(r':', '', tweet)\n",
        "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
        "    #replace consecutive non-ASCII characters with a space\n",
        "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
        " \n",
        " \n",
        "    #remove emojis from tweet\n",
        "    tweet = emoji_pattern.sub(r'', tweet)\n",
        " \n",
        "    #filter using NLTK library append it to a string\n",
        "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
        "    filtered_tweet = []\n",
        " \n",
        "    #looping through conditions\n",
        "    for w in word_tokens:\n",
        "        #check tokens against stop words , emoticons and punctuations\n",
        "        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
        "            filtered_tweet.append(w)\n",
        "    return ' '.join(filtered_tweet)\n",
        "    #print(word_tokens)\n",
        "    #print(filtered_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n6b_9DjDJLf"
      },
      "source": [
        "The below function writes the tweets into the CSV files. Textblob library is used to obtain the sentiment scores for our tweets. Textblob returns sentiment score in the range of -1 to +1. Where +1 indicates positive and -1 indicates negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaw8-LqRDJLg"
      },
      "outputs": [],
      "source": [
        "#method write_tweets()\n",
        "def write_tweets(keyword, file):\n",
        "    # If the file exists, then read the existing data from the CSV file.\n",
        "    if os.path.exists(file):\n",
        "        df = pd.read_csv(file, header=0)\n",
        "    else:\n",
        "        df = pd.DataFrame(columns=COLS)\n",
        "    #page attribute in tweepy.cursor and iteration\n",
        "    for page in tweepy.Cursor(api.search, q=keyword,\n",
        "                              count=200, include_rts=False, since=start_date).pages(50):\n",
        "        for status in page:\n",
        "            new_entry = []\n",
        "            status = status._json\n",
        " \n",
        "            ## check whether the tweet is in english or skip to the next tweet\n",
        "            if status['lang'] != 'en':\n",
        "                continue\n",
        " \n",
        "            #when run the code, below code replaces the retweet amount and\n",
        "            #no of favorires that are changed since last download.\n",
        "            if status['created_at'] in df['created_at'].values:\n",
        "                i = df.loc[df['created_at'] == status['created_at']].index[0]\n",
        "                if status['favorite_count'] != df.at[i, 'favorite_count'] or \\\n",
        "                   status['retweet_count'] != df.at[i, 'retweet_count']:\n",
        "                    df.at[i, 'favorite_count'] = status['favorite_count']\n",
        "                    df.at[i, 'retweet_count'] = status['retweet_count']\n",
        "                continue\n",
        " \n",
        " \n",
        "           #tweepy preprocessing called for basic preprocessing\n",
        "            #clean_text = p.clean(status['text'])\n",
        " \n",
        "            #call clean_tweet method for extra preprocessing\n",
        "            filtered_tweet=clean_tweets(status['text'])\n",
        " \n",
        "            #pass textBlob method for sentiment calculations\n",
        "            blob = TextBlob(filtered_tweet)\n",
        "            Sentiment = blob.sentiment\n",
        " \n",
        "            #seperate polarity and subjectivity in to two variables\n",
        "            polarity = Sentiment.polarity\n",
        "            subjectivity = Sentiment.subjectivity\n",
        " \n",
        "            #new entry append\n",
        "            new_entry += [status['id'], status['created_at'],\n",
        "                          status['source'], status['text'],filtered_tweet, Sentiment,polarity,subjectivity, status['lang'],\n",
        "                          status['favorite_count'], status['retweet_count']]\n",
        " \n",
        "            #to append original author of the tweet\n",
        "            new_entry.append(status['user']['screen_name'])\n",
        " \n",
        "            try:\n",
        "                is_sensitive = status['possibly_sensitive']\n",
        "            except KeyError:\n",
        "                is_sensitive = None\n",
        "            new_entry.append(is_sensitive)\n",
        " \n",
        "            # hashtagas and mentiones are saved using comma separted\n",
        "            hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n",
        "            new_entry.append(hashtags)\n",
        "            mentions = \", \".join([mention['screen_name'] for mention in status['entities']['user_mentions']])\n",
        "            new_entry.append(mentions)\n",
        " \n",
        "            #get location of the tweet if possible\n",
        "            try:\n",
        "                location = status['user']['location']\n",
        "            except TypeError:\n",
        "                location = ''\n",
        "            new_entry.append(location)\n",
        " \n",
        "            try:\n",
        "                coordinates = [coord for loc in status['place']['bounding_box']['coordinates'] for coord in loc]\n",
        "            except TypeError:\n",
        "                coordinates = None\n",
        "            new_entry.append(coordinates)\n",
        " \n",
        "            single_tweet_df = pd.DataFrame([new_entry], columns=COLS)\n",
        "            df = df.append(single_tweet_df, ignore_index=True)\n",
        "            csvFile = open(file, 'a' ,encoding='utf-8')\n",
        "    df.to_csv(csvFile, mode='a', columns=COLS, index=False, encoding=\"utf-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sPfrVtCDJLh"
      },
      "source": [
        "We obtains tweets by passing keywords. The API will fetch us tweets related to the keywords that we pass. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5Nf1IZ9DJLh"
      },
      "outputs": [],
      "source": [
        "#declare keywords as a query for three categories\n",
        "Congress_keywords = '#IndianNationalCongress OR #RahulGandhi OR #SoniaGandhi OR #INC' \n",
        "BJP_keywords = '#BJP OR #Modi OR #AmitShah OR #BhartiyaJantaParty'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw0IOdP3DJLi"
      },
      "outputs": [],
      "source": [
        "#call main method passing keywords and file path\n",
        "write_tweets(Congress_keywords,  congress_tweets)\n",
        "write_tweets(BJP_keywords, bjp_tweets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN0XtaOZDJLi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Election_dataScrape.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}